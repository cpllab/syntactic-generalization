{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import operator\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Load data and preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from test suite tag to high-level circuit.\n",
    "circuits = {\n",
    "    \"Licensing\": [\"npi\", \"reflexive\"],\n",
    "    \"Long-Distance Dependencies\": [\"fgd\", \"cleft\"],\n",
    "    \"Agreement\": [\"number\"],\n",
    "    \"Garden-Path Effects\": [\"npz\", \"mvrr\"],\n",
    "    \"Gross Syntactic State\": [\"subordination\"],\n",
    "    \"Center Embedding\": [\"center\"],\n",
    "}\n",
    "\n",
    "tag_to_circuit = {tag: circuit\n",
    "                  for circuit, tags in circuits.items()\n",
    "                  for tag in tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map codenames to readable names for various columns.\n",
    "def format_pretrained(model_name):\n",
    "    return \"%s$^*$\" % model_name\n",
    "\n",
    "PRETTY_COLUMN_MAPS = [\n",
    "    (\"model_name\",\n",
    "     {\n",
    "        \"tinylstm\": \"LSTM\",\n",
    "        \"ordered-neurons\": \"ON-LSTM\",\n",
    "        \"rnng\": \"RNNG\",\n",
    "        \"ngram\": \"n-gram\",\n",
    "        \"random\": \"Random\",\n",
    "         \n",
    "        \"gpt-2-pretrained\": format_pretrained(\"GPT-2\"),\n",
    "        \"gpt-2-xl-pretrained\": format_pretrained(\"GPT-2-XL\"),\n",
    "        \"gpt-2\": \"GPT-2\",\n",
    "        \"transformer-xl\": format_pretrained(\"Transformer-XL\"),\n",
    "        \"grnn\": format_pretrained(\"GRNN\"),\n",
    "        \"jrnn\": format_pretrained(\"JRNN\"),\n",
    "    }),\n",
    "    \n",
    "    (\"corpus\", lambda x: x.upper() if x else \"N/A\"),\n",
    "]\n",
    "PRETTY_COLUMNS = [\"pretty_%s\" % col for col, _ in PRETTY_COLUMN_MAPS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclusions\n",
    "exclude_suite_re = re.compile(r\"^fgd-embed[34]|^gardenpath|^nn-nv\")\n",
    "exclude_models = [\"1gram\", \"ngram-no-rand\"] # \"ngram\", "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_models = [\"1gram\", \"ngram\", \"ngram-single\"]\n",
    "baseline_models = [\"random\"]\n",
    "\n",
    "# Models for which we designed a controlled training regime\n",
    "controlled_models = [\"ngram\", \"ordered-neurons\", \"tinylstm\", \"rnng\", \"gpt-2\"]\n",
    "controlled_nonbpe_models = [\"ngram\", \"ordered-neurons\", \"tinylstm\", \"rnng\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_data_path = Path(\"../data/raw/perplexity.csv\")\n",
    "test_suite_results_path = Path(\"../output/all_results.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_df = pd.read_csv(ppl_data_path, index_col=[\"model\", \"corpus\", \"seed\"])\n",
    "perplexity_df.index.set_names(\"model_name\", level=0, inplace=True)\n",
    "\n",
    "results_df = pd.read_csv(test_suite_results_path, delim_whitespace=True)\n",
    "results_df[\"seed\"] = results_df.seed.fillna(\"0\").astype(int)\n",
    "results_df = results_df.rename(columns=dict(item_number=\"item\", result=\"correct\"))\n",
    "\n",
    "# Add tags\n",
    "results_df[\"tag\"] = results_df.suite.transform(lambda s: re.split(r\"[-_0-9]\", s)[0])\n",
    "results_df[\"circuit\"] = results_df.tag.map(tag_to_circuit)\n",
    "tags_missing_circuit = set(results_df.tag.unique()) - set(tag_to_circuit.keys())\n",
    "if tags_missing_circuit:\n",
    "    print(\"Tags missing circuit: \", \", \".join(tags_missing_circuit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude test suites\n",
    "exclude_filter = results_df.suite.str.contains(exclude_suite_re)\n",
    "print(\"Dropping %i results / %i suites due to exclusions:\"\n",
    "      % (exclude_filter.sum(), len(results_df[exclude_filter].suite.unique())))\n",
    "print(\" \".join(results_df[exclude_filter].suite.unique()))\n",
    "results_df = results_df[~exclude_filter]\n",
    "\n",
    "# Exclude models\n",
    "exclude_filter = results_df.model_name.isin(exclude_models)\n",
    "print(\"Dropping %i results due to dropping models:\" % exclude_filter.sum(), list(results_df[exclude_filter].model_name.unique()))\n",
    "results_df = results_df[~exclude_filter]\n",
    "\n",
    "# Exclude word-level controlled models with BPE tokenization\n",
    "exclude_filter = (results_df.model_name.isin(controlled_nonbpe_models)) & (results_df.corpus.str.endswith(\"bpe\"))\n",
    "results_df = results_df[~exclude_filter]\n",
    "\n",
    "# Exclude GPT-2 with word-level or SentencePieceBPE tokenization\n",
    "exclude_filter = ((results_df.model_name==\"gpt-2\") & ~(results_df.corpus.str.endswith(\"gptbpe\")))\n",
    "results_df = results_df[~exclude_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average across seeds of each ngram model.\n",
    "# The only difference between \"seeds\" of these model types are random differences in tie-breaking decisions.\n",
    "for ngram_model in ngram_models:\n",
    "    # Create a synthetic results_df with one ngram model, where each item is correct if more than half of\n",
    "    # the ngram seeds vote.\n",
    "    ngram_results_df = (results_df[results_df.model_name == ngram_model].copy()\n",
    "                        .groupby([\"model_name\", \"corpus\", \"suite\", \"item\", \"tag\", \"circuit\"])\n",
    "                        .agg({\"correct\": \"mean\"}) > 0.5).reset_index()\n",
    "    ngram_results_df[\"seed\"] = 0\n",
    "    \n",
    "    # Drop existing model results.\n",
    "    results_df = pd.concat([results_df[~(results_df.model_name == ngram_model)],\n",
    "                            ngram_results_df], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettify name columns, which we'll carry through data manipulations\n",
    "for column, map_fn in PRETTY_COLUMN_MAPS:\n",
    "    pretty_column = \"pretty_%s\" % column\n",
    "    results_df[pretty_column] = results_df[column].map(map_fn)\n",
    "    if results_df[pretty_column].isna().any():\n",
    "        print(\"WARNING: In prettifying %s, yielded NaN values:\" % column)\n",
    "        print(results_df[results_df[pretty_column].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suites_df = results_df.groupby([\"model_name\", \"corpus\", \"seed\", \"suite\"] + PRETTY_COLUMNS).correct.mean().reset_index()\n",
    "suites_df[\"tag\"] = suites_df.suite.transform(lambda s: re.split(r\"[-_0-9]\", s)[0])\n",
    "suites_df[\"circuit\"] = suites_df.tag.map(tag_to_circuit)\n",
    "\n",
    "# For controlled evaluation:\n",
    "# Compute a model's test suite accuracy relative to the mean accuracy on this test suite.\n",
    "# Only compute this on controlled models.\n",
    "def get_controlled_mean(suite_results):\n",
    "    # When computing test suite mean, first collapse test suite accuracies within model--corpus, then combine resulting means.\n",
    "    return suite_results[suite_results.model_name.isin(controlled_models)].groupby([\"model_name\", \"corpus\"]).correct.mean().mean()\n",
    "suite_means = suites_df.groupby(\"suite\").apply(get_controlled_mean)\n",
    "suites_df[\"correct_delta\"] = suites_df.apply(lambda r: r.correct - suite_means.loc[r.suite] if r.model_name in controlled_models else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll save this data to a CSV file for access from R, where we do\n",
    "# linear mixed-effects regression modeling.\n",
    "suites_df.to_csv(\"../data/suites_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join PPL and accuracy data.\n",
    "joined_data = suites_df.groupby([\"model_name\", \"corpus\", \"seed\"] + PRETTY_COLUMNS)[[\"correct\", \"correct_delta\"]].agg(\"mean\")\n",
    "joined_data = pd.DataFrame(joined_data).join(perplexity_df).reset_index()\n",
    "joined_data.head()\n",
    "\n",
    "# Track BPE + size separately.\n",
    "joined_data[\"corpus_size\"] = joined_data.corpus.str.split(\"-\").apply(lambda tokens: tokens[1] if len(tokens) >= 2 else None)\n",
    "joined_data[\"corpus_bpe\"] = joined_data.corpus.str.split(\"-\").apply(lambda tokens: tokens[2] if len(tokens) > 2 else (\"none\" if len(tokens) >= 2 else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join PPL and accuracy data, splitting on circuit.\n",
    "joined_data_circuits = suites_df.groupby([\"model_name\", \"corpus\", \"seed\", \"circuit\"] + PRETTY_COLUMNS)[[\"correct\", \"correct_delta\"]].agg(\"mean\")\n",
    "joined_data_circuits = pd.DataFrame(joined_data_circuits).reset_index().set_index([\"model_name\", \"corpus\", \"seed\"]).join(perplexity_df).reset_index()\n",
    "joined_data_circuits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze stability to modification.\n",
    "def has_modifier(ts):\n",
    "    if ts.endswith((\"_modifier\", \"_mod\")):\n",
    "        return True\n",
    "    else:\n",
    "        return None\n",
    "suites_df[\"has_modifier\"] = suites_df.suite.transform(has_modifier)\n",
    "\n",
    "# Mark \"non-modifier\" test suites\n",
    "modifier_ts = suites_df[suites_df.has_modifier == True].suite.unique()\n",
    "no_modifier_ts = [re.sub(r\"_mod(ifier)?$\", \"\", ts) for ts in modifier_ts]\n",
    "suites_df.loc[suites_df.suite.isin(no_modifier_ts), \"has_modifier\"] = False\n",
    "# Store subset of test suites which have definite modifier/no-modifier marking\n",
    "suites_df_mod = suites_df[~(suites_df.has_modifier.isna())].copy()\n",
    "suites_df_mod[\"has_modifier\"] = suites_df_mod.has_modifier.astype(bool)\n",
    "# Get base test suite (without modifier/no-modifier marking)\n",
    "suites_df_mod[\"test_suite_base\"] = suites_df_mod.suite.transform(lambda ts: ts.strip(\"_no-modifier\").strip(\"_modifier\"))\n",
    "suites_df_mod.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Each model--corpus--seed should have perplexity data.\n",
    "ids_from_results = results_df.set_index([\"model_name\", \"corpus\", \"seed\"]).sort_index().index\n",
    "ids_from_ppl = perplexity_df.sort_index().index\n",
    "diff = set(ids_from_results) - set(ids_from_ppl)\n",
    "if diff:\n",
    "    print(\"Missing perplexity results for:\")\n",
    "    pprint(diff)\n",
    "    #raise ValueError(\"Each model--corpus--seed must have perplexity data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every model--corpus--seed should have results for all test suite items.\n",
    "item_list = {model_key: set(results.suite)\n",
    "             for model_key, results in results_df.groupby([\"model_name\", \"corpus\", \"seed\"])}\n",
    "not_shared = set()\n",
    "for k1, k2 in itertools.combinations(item_list.keys(), 2):\n",
    "    l1, l2 = item_list[k1], item_list[k2]\n",
    "    if l1 != l2:\n",
    "        print(\"SyntaxGym test suite results for %s and %s don't match\" % (k1, k2))\n",
    "        print(\"\\tIn %s but not in %s:\\n\\t\\t%s\" % (k2, k1, l2 - l1))\n",
    "        print(\"\\tIn %s but not in %s:\\n\\t\\t%s\" % (k1, k2, l1 - l2))\n",
    "        print()\n",
    "        \n",
    "        not_shared |= l2 - l1\n",
    "        not_shared |= l1 - l2\n",
    "\n",
    "if len(not_shared) > 0:\n",
    "    to_drop = results_df[results_df.suite.isin(not_shared)]\n",
    "    print(\"Dropping these test suites (%i rows) for now. Yikes:\" % len(to_drop))\n",
    "    print(not_shared)\n",
    "    results_df = results_df[~results_df.suite.isin(not_shared)]\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second sanity check: same number of results per model--corpus--seed\n",
    "result_counts = results_df.groupby([\"model_name\", \"corpus\", \"seed\"]).item.count()\n",
    "if len(result_counts.unique()) > 1:\n",
    "    print(\"WARNING: Some model--corpus--seed combinations have more result rows in results_df than others.\")\n",
    "    print(result_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second sanity check: same number of suite-level results per model--corpus--seed\n",
    "suite_result_counts = suites_df.groupby([\"model_name\", \"corpus\", \"seed\"]).suite.count()\n",
    "if len(suite_result_counts.unique()) > 1:\n",
    "    print(\"WARNING: Some model--corpus--seed combinations have more result rows in suites_df than others.\")\n",
    "    print(suite_result_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for data rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_FINAL = True\n",
    "figure_path = Path(\"../reports/camera_ready_figures\")\n",
    "figure_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "RENDER_CONTEXT = {\n",
    "    \"font_scale\": 3.5,\n",
    "    \"rc\": {\"lines.linewidth\": 2.5, \"hatch.linewidth\":3},\n",
    "    \"style\": \"ticks\",\n",
    "    \"font\": \"Liberation Sans\"\n",
    "}\n",
    "sns.set(**RENDER_CONTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_LINESTYLE = {\n",
    "    \"color\": \"gray\",\n",
    "    \"linestyle\": \"--\",\n",
    "}\n",
    "CORPUS_MARKERS = {\n",
    "    \"BLLIP-LG\": \"s\",\n",
    "    \"BLLIP-MD\": \"v\",\n",
    "    \"BLLIP-SM\": \"P\",\n",
    "    \"BLLIP-XS\": \"X\",\n",
    "    \n",
    "    \"BLLIP-LG-BPE\": \"s\",\n",
    "    \"BLLIP-MD-BPE\": \"v\",\n",
    "\n",
    "    \"BLLIP-LG-GPTBPE\": \"s\",\n",
    "    \"BLLIP-MD-GPTBPE\": \"v\",\n",
    "    \"BLLIP-SM-GPTBPE\": \"P\",\n",
    "    \"BLLIP-XS-GPTBPE\": \"X\"\n",
    "}\n",
    "p = sns.color_palette()#[:len(joined_data.model_name.unique())]\n",
    "MODEL_COLORS = {\n",
    "    \"GPT-2\": p[0],\n",
    "    \"LSTM\": p[1],\n",
    "    \"ON-LSTM\": p[2],\n",
    "    \"RNNG\": p[3],\n",
    "    \"n-gram\": p[4],\n",
    "    \"Random\": \"darkgrey\",\n",
    "         \n",
    "    format_pretrained(\"GPT-2\"): \"mediumturquoise\",\n",
    "    format_pretrained(\"GPT-2-XL\"): p[5],\n",
    "    format_pretrained(\"Transformer-XL\"): \"gold\",\n",
    "    format_pretrained(\"GRNN\"): p[6],\n",
    "    format_pretrained(\"JRNN\"):  \"deeppink\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_final(path):\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize axis labels\n",
    "SG_ABSOLUTE_LABEL = \"SG score\"\n",
    "SG_DELTA_LABEL = \"SG score delta\"\n",
    "PERPLEXITY_LABEL = \"Test perplexity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish consistent orderings of model names, corpus names, circuit names\n",
    "# for figure ordering / coloring. (NB these refer to prettified names)\n",
    "model_order = sorted(set(results_df.pretty_model_name))\n",
    "controlled_model_order = [\"LSTM\", \"ON-LSTM\", \"RNNG\", \"GPT-2\", \"n-gram\"] #sorted(set(results_df[results_df.model_name.isin(controlled_models)].pretty_model_name))\n",
    "corpus_order = [\"BLLIP-LG\", \"BLLIP-MD\", \"BLLIP-SM\", \"BLLIP-XS\",\n",
    "                \"BLLIP-LG-BPE\", \"BLLIP-LG-GPTBPE\", \n",
    "                \"BLLIP-MD-GPTBPE\", \"BLLIP-SM-GPTBPE\", \"BLLIP-XS-GPTBPE\"]\n",
    "corpus_size_order = [\"lg\", \"md\", \"sm\", \"xs\"]\n",
    "nobpe_corpus_order = [c for c in corpus_order if \"BPE\" not in c]\n",
    "circuit_order = sorted([c for c in results_df.circuit.dropna().unique()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing paper figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1 (Basic barplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# Exclude random baseline; will plot as horizontal line\n",
    "plot_df = suites_df[(suites_df.model_name != \"random\")]\n",
    "\n",
    "# Sort by decreasing average accuracy\n",
    "order = list(plot_df.groupby(\"pretty_model_name\").correct.mean().sort_values(ascending=False).index)\n",
    "\n",
    "sns.barplot(data=plot_df.reset_index(), x=\"pretty_model_name\", y=\"correct\", order=order, ax=ax, palette=MODEL_COLORS)\n",
    "\n",
    "# Plot random chance baseline\n",
    "ax.axhline(suites_df[suites_df.model_name == \"random\"].correct.mean(), **BASELINE_LINESTYLE)\n",
    "\n",
    "# Adjust labels and axes\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=340, horizontalalignment=\"left\")\n",
    "ax.set_ylim(0,1)\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(SG_ABSOLUTE_LABEL, labelpad=36)\n",
    "\n",
    "if RENDER_FINAL:\n",
    "    render_final(figure_path / \"overall.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlled evaluation of model type + dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlled_suites_df = suites_df[suites_df.model_name.isin(controlled_models)]\n",
    "controlled_suites_df_mod = suites_df_mod[suites_df_mod.model_name.isin(controlled_models)]\n",
    "controlled_joined_data_circuits = joined_data_circuits[joined_data_circuits.model_name.isin(controlled_models)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=True, figsize=(40,12))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.axhline(0, c=\"gray\", linestyle=\"--\")\n",
    "    if i == 0:\n",
    "        kwargs = dict(data=controlled_suites_df.reset_index(), order=controlled_model_order, ax=ax,\n",
    "                      x=\"pretty_model_name\", y=\"correct_delta\", palette=MODEL_COLORS)\n",
    "        sns.barplot(**kwargs, units=\"corpus\")\n",
    "        sns.swarmplot(**kwargs, alpha=0.3, size=9)\n",
    "\n",
    "        ax.set_xlabel(\"Model\", labelpad=16)\n",
    "        ax.set_ylabel(SG_DELTA_LABEL)\n",
    "    elif i == 1:\n",
    "        # Estimate error intervals with a structured bootstrap: resampling units = model\n",
    "        kwargs = dict(data=controlled_suites_df.reset_index(), x=\"pretty_corpus\", y=\"correct_delta\", order=nobpe_corpus_order, ax=ax)\n",
    "        sns.barplot(**kwargs, color=\"Gray\", units=\"pretty_model_name\")\n",
    "        sns.swarmplot(**kwargs, hue=\"pretty_model_name\",  hue_order=controlled_model_order, palette=MODEL_COLORS, size=9, alpha=0.5)\n",
    "\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        for h in handles:\n",
    "            h.set_sizes([300.0])\n",
    "\n",
    "        ax.set_xlabel(\"Corpus\", labelpad=16)\n",
    "        ax.set_ylabel(\"\")\n",
    "        ax.legend(handles, labels, loc=\"upper center\", ncol=5, columnspacing=0.3, handletextpad=0.01)\n",
    "\n",
    "if RENDER_FINAL:\n",
    "    render_final(figure_path / \"controlled.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, figsize=(40,15))\n",
    "legend_params=dict(title=\"\", ncol=5, loc=\"upper center\", columnspacing=1, handlelength=1, handletextpad=0.3)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.axhline(0, **BASELINE_LINESTYLE)\n",
    "    if i == 0:\n",
    "        sns.barplot(data=controlled_joined_data_circuits, x=\"circuit\", y=\"correct_delta\",\n",
    "                    hue=\"pretty_model_name\", units=\"corpus\", hue_order=controlled_model_order,\n",
    "                    ax=ax, palette=MODEL_COLORS)\n",
    "        ax.set_ylabel(SG_DELTA_LABEL)\n",
    "    elif i == 1:\n",
    "        sns.barplot(data=controlled_joined_data_circuits, x=\"circuit\", y=\"correct_delta\",\n",
    "                    hue=\"pretty_corpus\", units=\"model_name\", hue_order=nobpe_corpus_order,\n",
    "                    ax=ax, palette=\"Greys_r\")\n",
    "        ax.set_ylabel(\"\")\n",
    "        \n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=15, ha=\"right\")\n",
    "    ax.set_xlabel(\"Circuit\")\n",
    "    ax.legend(**legend_params)\n",
    "\n",
    "if RENDER_FINAL:\n",
    "    render_final(figure_path / \"controlled_circuit.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(40,12))\n",
    "joined_data_circuits_norandom = joined_data_circuits[joined_data_circuits.pretty_model_name != \"Random\"]\n",
    "order = list(plot_df.groupby(\"pretty_model_name\").correct.mean().sort_values(ascending=False).index)\n",
    "sns.barplot(data=joined_data_circuits_norandom, x=\"circuit\", y=\"correct\",\n",
    "            hue=\"pretty_model_name\", units=\"corpus\", hue_order=order, ax=ax, palette=MODEL_COLORS)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=15, ha=\"right\")\n",
    "ax.set_xlabel(\"Circuit\")\n",
    "ax.set_ylabel(SG_ABSOLUTE_LABEL)\n",
    "ax.legend(title=\"\", ncol=int(len(order)/2), loc=\"upper center\", columnspacing=1, handlelength=1, handletextpad=1, bbox_to_anchor=(0.5,1.3))\n",
    "\n",
    "if RENDER_FINAL:\n",
    "    render_final(figure_path / \"allmodels_circuit.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 6 (Stability to modification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Suites in modification analysis:\", controlled_suites_df_mod.suite.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by decreasing average accuracy.\n",
    "order = list(plot_df.groupby(\"pretty_model_name\").correct.mean().sort_values(ascending=False).index)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(20,12))\n",
    "sns.barplot(data=suites_df_mod, x=\"pretty_model_name\", y=\"correct\", hue=\"has_modifier\", order=order, ax=ax)\n",
    "\n",
    "# Colors.\n",
    "sorted_patches = sorted(ax.patches, key=lambda bar: bar.get_x())\n",
    "colors = [MODEL_COLORS[order[i]] for i in range(len(order))]\n",
    "for i, bar in enumerate(sorted_patches):\n",
    "    bar.set_facecolor(colors[int(i/2)])\n",
    "    if i % 2 != 0:\n",
    "        bar.set_alpha(0.4)\n",
    "\n",
    "# Set labels.\n",
    "ax.set_xlabel(\"Model\", labelpad=16)\n",
    "ax.set_ylabel(SG_ABSOLUTE_LABEL)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=340, horizontalalignment=\"left\")\n",
    "\n",
    "# Custom legend.\n",
    "handles, _ = ax.get_legend_handles_labels()\n",
    "handles[0] = mpatches.Patch(facecolor=\"k\")\n",
    "handles[1] = mpatches.Patch(facecolor=\"k\", alpha=0.4)\n",
    "ax.legend(handles, [\"No modifier\", \"With modifier\"], loc=\"upper right\", title=\"\")\n",
    "\n",
    "if RENDER_FINAL:\n",
    "    render_final(figure_path / \"stability-all-models.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2 (SG score vs perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set limits for broken x-axis to  determine proper scaling (ratio of widths).\n",
    "ax1max = 250\n",
    "ax2min, ax2max = 520, 540\n",
    "ax_ratio = ax1max / (ax2max - ax2min)\n",
    "f, (ax1,ax2) = plt.subplots(1,2,sharey=False,figsize=(19, 20),gridspec_kw={'width_ratios': [ax_ratio, 1]})\n",
    "\n",
    "sns.despine()\n",
    "palette = sns.cubehelix_palette(4, reverse=True)\n",
    "\n",
    "markers = {\n",
    "    \"GPT-2\": \"s\",\n",
    "    \"RNNG\" : \"X\",\n",
    "    \"ON-LSTM\" : \"v\",\n",
    "    \"LSTM\" : \"*\",\n",
    "    \"n-gram\" : \"d\"\n",
    "}\n",
    "for m in joined_data.pretty_model_name.unique():\n",
    "    if m not in markers:\n",
    "        markers[m] = \".\"\n",
    "\n",
    "for ax in [ax1,ax2]:\n",
    "    sns.scatterplot(data=joined_data, x=\"test_ppl\", y=\"correct\", hue=\"corpus_size\", hue_order=corpus_size_order,\n",
    "                    markers=markers, palette=palette, style_order=model_order,\n",
    "                    s=2300, style=\"pretty_model_name\", ax=ax, zorder=2, alpha=0.8)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.tick_params(axis='x', which='major', pad=15)\n",
    "\n",
    "    # Add horizontal lines for models without ppl estimates.\n",
    "    no_ppl_data = joined_data[joined_data.test_ppl.isna()]\n",
    "    for model_name, rows in no_ppl_data.groupby(\"pretty_model_name\"):\n",
    "        y = rows.correct.mean()\n",
    "        ax.axhline(y, zorder=1, linewidth=3, **BASELINE_LINESTYLE) \n",
    "        if \"GRNN\" in model_name: # custom spacing tweaking\n",
    "            y_offset = -0.03\n",
    "        else:\n",
    "            y_offset = 0.006\n",
    "        ax2.text(540, y + y_offset, model_name, fontdict={\"size\": 38}, ha='right')\n",
    "    \n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "ax1.get_legend().remove()\n",
    "ax1.set_ylabel(SG_ABSOLUTE_LABEL)\n",
    "ax2.set_ylabel(\"\")\n",
    "plt.xlabel(PERPLEXITY_LABEL, labelpad=10, position=(-6,0))\n",
    "\n",
    "# Add break in x-axis\n",
    "ax1.set_xlim(0,ax1max)\n",
    "ax2.set_xlim(ax2min,ax2max)\n",
    "# hide the spines between ax1 and ax2\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax2.spines['left'].set_visible(False)\n",
    "ax2.get_yaxis().set_ticks([])\n",
    "d = .015 # how big to make the diagonal lines in axes coordinates\n",
    "kwargs = dict(transform=ax1.transAxes, color='k', clip_on=False)\n",
    "ax1.plot((1-d,1+d), (-d,+d), **kwargs)\n",
    "kwargs.update(transform=ax2.transAxes)  # switch to the right subplot\n",
    "ax2.plot((-d*ax_ratio,+d*ax_ratio), (-d,+d), **kwargs)\n",
    "    \n",
    "# Change some legend labels.\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "legend_title_map = {\"pretty_model_name\": \"Model\",\n",
    "                    \"pretty_corpus\": \"Corpus\",\n",
    "                    \"corpus_size\": \"Corpus size\",\n",
    "                    \"corpus_bpe\": \"Tokenization\"}\n",
    "# Re-map some labels.\n",
    "# labels = [legend_title_map.get(l, l) for l in labels]\n",
    "drop_indices = [i for i,l in enumerate(labels) if l in legend_title_map.keys() or l in no_ppl_data.pretty_model_name.values]\n",
    "handles = [h for i,h in enumerate(handles) if i not in drop_indices]\n",
    "labels = [l for i,l in enumerate(labels) if i not in drop_indices]\n",
    "labels = [l if l not in joined_data.corpus_size.unique() else \"BLLIP-%s\" % l.upper() for l in labels]\n",
    "\n",
    "# Add empty handle for legend spacing.\n",
    "handles.insert(4, mpatches.Patch(facecolor=\"white\"))\n",
    "labels.insert(4, \"\")\n",
    "\n",
    "# Re-order labels.\n",
    "new_order = [\"BLLIP-LG\", \"LSTM\", \"BLLIP-MD\", \"ON-LSTM\", \"BLLIP-SM\", \"RNNG\", \"BLLIP-XS\", \"GPT-2\", \"\", \"n-gram\"]\n",
    "inds = [labels.index(l) for l in new_order]\n",
    "handles = [handles[i] for i in inds]\n",
    "labels = [labels[i] for i in inds]\n",
    "\n",
    "# Set model style markers in legend to outlines only.\n",
    "for i, (l, h) in enumerate(zip(labels, handles)):\n",
    "    if l != \"\":\n",
    "        h.set_sizes([500.0])\n",
    "        if l in joined_data.pretty_model_name.unique():\n",
    "             handles[i] = Line2D([0], [0], marker=markers[l], color='k', mew=3, lw=0,\n",
    "                          markerfacecolor='w', markersize=27)\n",
    "\n",
    "plt.legend(handles, labels, bbox_to_anchor=(-16.4,-0.18), ncol=5, loc=\"center left\", columnspacing=0.5, handletextpad=0.05)\n",
    "\n",
    "if RENDER_FINAL:\n",
    "    # Can't use render_final function because of some spine issues.\n",
    "    plt.savefig(figure_path / \"perplexity.pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
