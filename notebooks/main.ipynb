{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import operator\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Load data and preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from test suite tag to high-level circuit.\n",
    "circuits = {\n",
    "    \"Licensing\": [\"npi\", \"reflexive\"],\n",
    "    \"Long-Distance Dependencies\": [\"fgd\", \"cleft\"],\n",
    "    \"Agreement\": [\"number\"],\n",
    "    \"Garden-Path Effects\": [\"npz\", \"mvrr\"],\n",
    "    \"Gross Syntactic State\": [\"subordination\"],\n",
    "    \"Center Embedding\": [\"center\"],\n",
    "}\n",
    "\n",
    "tag_to_circuit = {tag: circuit\n",
    "                  for circuit, tags in circuits.items()\n",
    "                  for tag in tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclusions\n",
    "exclude_suite_re = re.compile(r\"^fgd-embed[34]|^gardenpath|^nn-nv\")\n",
    "exclude_models = [\"1gram\", \"ngram\", \"ngram-no-rand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_models = [\"1gram\", \"ngram\", \"ngram-single\"]\n",
    "baseline_models = [\"random\"]\n",
    "\n",
    "# Models for which we designed a controlled training regime\n",
    "controlled_models = [\"ngram\", \"ordered-neurons\", \"vanilla\", \"rnng\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_data_path = Path(\"../data/raw/perplexity.csv\")\n",
    "test_suite_results_path = Path(\"../data/raw/test_suite_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_df = pd.read_csv(ppl_data_path, index_col=[\"model\", \"corpus\", \"seed\"])\n",
    "perplexity_df.index.set_names(\"model_name\", level=0, inplace=True)\n",
    "\n",
    "results_df = pd.concat([pd.read_csv(f) for f in test_suite_results_path.glob(\"*.csv\")])\n",
    "\n",
    "# Split model_id into constituent parts\n",
    "model_ids = results_df.model.str.split(\"_\", expand=True).rename(columns={0: \"model_name\", 1: \"corpus\", 2: \"seed\"})\n",
    "results_df = pd.concat([results_df, model_ids], axis=1).drop(columns=[\"model\"])\n",
    "results_df[\"seed\"] = results_df.seed.fillna(\"0\").astype(int)\n",
    "\n",
    "# Exclude test suites\n",
    "exclude_filter = results_df.suite.str.contains(exclude_suite_re)\n",
    "print(\"Dropping %i results / %i suites due to exclusions:\"\n",
    "      % (exclude_filter.sum(), len(results_df[exclude_filter].suite.unique())))\n",
    "print(\" \".join(results_df[exclude_filter].suite.unique()))\n",
    "results_df = results_df[~exclude_filter]\n",
    "\n",
    "# Exclude models\n",
    "exclude_filter = results_df.model_name.isin(exclude_models)\n",
    "print(\"Dropping %i results due to dropping models:\" % exclude_filter.sum(), list(results_df[exclude_filter].model_name.unique()))\n",
    "results_df = results_df[~exclude_filter]\n",
    "\n",
    "# Add tags\n",
    "results_df[\"tag\"] = results_df.suite.transform(lambda s: re.split(r\"[-_0-9]\", s)[0])\n",
    "results_df[\"circuit\"] = results_df.tag.map(tag_to_circuit)\n",
    "tags_missing_circuit = set(results_df.tag.unique()) - set(tag_to_circuit.keys())\n",
    "if tags_missing_circuit:\n",
    "    print(\"Tags missing circuit: \", \", \".join(tags_missing_circuit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average across seeds of each ngram model.\n",
    "# The only difference between \"seeds\" of these model types are random differences in tie-breaking decisions.\n",
    "for ngram_model in ngram_models:\n",
    "    # Create a synthetic results_df with one ngram model, where each item is correct if more than half of\n",
    "    # the ngram seeds vote.\n",
    "    ngram_results_df = (results_df[results_df.model_name == ngram_model].copy().groupby([\"model_name\", \"corpus\", \"suite\", \"item\", \"tag\", \"circuit\"]).agg({\"correct\": \"mean\"}) > 0.5).reset_index()\n",
    "    ngram_results_df[\"seed\"] = 0\n",
    "    \n",
    "    # Drop existing model results.\n",
    "    results_df = pd.concat([results_df[~(results_df.model_name == ngram_model)],\n",
    "                            ngram_results_df], sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suites_df = results_df.groupby([\"model_name\", \"corpus\", \"seed\", \"suite\"]).correct.mean().reset_index()\n",
    "suites_df[\"tag\"] = suites_df.suite.transform(lambda s: re.split(r\"[-_0-9]\", s)[0])\n",
    "suites_df[\"circuit\"] = suites_df.tag.map(tag_to_circuit)\n",
    "\n",
    "# For controlled evaluation:\n",
    "# Compute a model's test suite accuracy relative to the mean accuracy on this test suite.\n",
    "# Only compute this on controlled models.\n",
    "def get_controlled_mean(suite_results):\n",
    "    # When computing test suite mean, first collapse test suite accuracies within model--corpus, then combine resulting means.\n",
    "    return suite_results[suite_results.model_name.isin(controlled_models)].groupby([\"model_name\", \"corpus\"]).correct.mean().mean()\n",
    "suite_means = suites_df.groupby(\"suite\").apply(get_controlled_mean)\n",
    "suites_df[\"correct_delta\"] = suites_df.apply(lambda r: r.correct - suite_means.loc[r.suite] if r.model_name in controlled_models else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join PPL and accuracy data.\n",
    "joined_data = suites_df.groupby([\"model_name\", \"corpus\", \"seed\"])[[\"correct\", \"correct_delta\"]].agg(\"mean\")\n",
    "joined_data = pd.DataFrame(joined_data).join(perplexity_df).reset_index()\n",
    "joined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join PPL and accuracy data, splitting on circuit.\n",
    "joined_data_circuits = suites_df.groupby([\"model_name\", \"corpus\", \"seed\", \"circuit\"])[[\"correct\", \"correct_delta\"]].agg(\"mean\")\n",
    "joined_data_circuits = pd.DataFrame(joined_data_circuits).reset_index().set_index([\"model_name\", \"corpus\", \"seed\"]).join(perplexity_df).reset_index()\n",
    "joined_data_circuits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze stability to modification.\n",
    "def has_modifier(ts):\n",
    "    if ts.endswith((\"_modifier\", \"_mod\")):\n",
    "        return True\n",
    "    else:\n",
    "        return None\n",
    "suites_df[\"has_modifier\"] = suites_df.suite.transform(has_modifier)\n",
    "\n",
    "# Mark \"non-modifier\" test suites\n",
    "modifier_ts = suites_df[suites_df.has_modifier == True].suite.unique()\n",
    "no_modifier_ts = [re.sub(r\"_mod(ifier)?$\", \"\", ts) for ts in modifier_ts]\n",
    "suites_df.loc[suites_df.suite.isin(no_modifier_ts), \"has_modifier\"] = False\n",
    "# Store subset of test suites which have definite modifier/no-modifier marking\n",
    "suites_df_mod = suites_df[~(suites_df.has_modifier.isna())].copy()\n",
    "# Get base test suite (without modifier/no-modifier marking)\n",
    "suites_df_mod[\"test_suite_base\"] = suites_df_mod.suite.transform(lambda ts: ts.strip(\"_no-modifier\").strip(\"_modifier\"))\n",
    "suites_df_mod.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Each model--corpus--seed should have perplexity data.\n",
    "ids_from_results = results_df.set_index([\"model_name\", \"corpus\", \"seed\"]).sort_index().index\n",
    "ids_from_ppl = perplexity_df.sort_index().index\n",
    "diff = set(ids_from_results) - set(ids_from_ppl)\n",
    "if diff:\n",
    "    print(\"Missing perplexity results for:\")\n",
    "    pprint(diff)\n",
    "    #raise ValueError(\"Each model--corpus--seed must have perplexity data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every model--corpus--seed should have results for all test suite items.\n",
    "item_list = {model_key: set(results.suite)\n",
    "             for model_key, results in results_df.groupby([\"model_name\", \"corpus\", \"seed\"])}\n",
    "not_shared = set()\n",
    "for k1, k2 in itertools.combinations(item_list.keys(), 2):\n",
    "    l1, l2 = item_list[k1], item_list[k2]\n",
    "    if l1 != l2:\n",
    "        print(\"SyntaxGym test suite results for %s and %s don't match\" % (k1, k2))\n",
    "        print(\"\\tIn %s but not in %s:\\n\\t\\t%s\" % (k2, k1, l2 - l1))\n",
    "        print(\"\\tIn %s but not in %s:\\n\\t\\t%s\" % (k1, k2, l1 - l2))\n",
    "        print()\n",
    "        \n",
    "        not_shared |= l2 - l1\n",
    "        not_shared |= l1 - l2\n",
    "\n",
    "if len(not_shared) > 0:\n",
    "    to_drop = results_df[results_df.suite.isin(not_shared)]\n",
    "    print(\"Dropping these test suites (%i rows) for now. Yikes:\" % len(to_drop))\n",
    "    print(not_shared)\n",
    "    results_df = results_df[~results_df.suite.isin(not_shared)]\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second sanity check: same number of results per model--corpus--seed\n",
    "result_counts = results_df.groupby([\"model_name\", \"corpus\", \"seed\"]).item.count()\n",
    "if len(result_counts.unique()) > 1:\n",
    "    print(\"WARNING: Some model--corpus--seed combinations have more result rows in results_df than others.\")\n",
    "    print(result_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second sanity check: same number of suite-level results per model--corpus--seed\n",
    "suite_result_counts = suites_df.groupby([\"model_name\", \"corpus\", \"seed\"]).suite.count()\n",
    "if len(suite_result_counts.unique()) > 1:\n",
    "    print(\"WARNING: Some model--corpus--seed combinations have more result rows in suites_df than others.\")\n",
    "    print(suite_result_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for data rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_FINAL = True\n",
    "figure_path = Path(\"../reports/figures\")\n",
    "figure_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "RENDER_CONTEXT = {\n",
    "    \"font_scale\": 3.5,\n",
    "    \"rc\": {\"lines.linewidth\": 2.5}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish consistent orderings of model names, corpus names, circuit names\n",
    "# for figure ordering / coloring\n",
    "model_order = sorted(set(results_df.model_name))\n",
    "controlled_model_order = sorted(set(results_df.model_name) & set(controlled_models))\n",
    "corpus_order = [\"bllip-lg\", \"bllip-md\", \"bllip-sm\", \"bllip-xs\"]\n",
    "circuit_order = sorted([c for c in results_df.circuit.dropna().unique()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines_to_plot = set(baseline_models) & set(suites_df.model_name.unique())\n",
    "f, axs = plt.subplots(len(baselines_to_plot), 1, figsize=(40, 10 * len(baselines_to_plot)))\n",
    "\n",
    "for baseline_model, ax in zip(baselines_to_plot, np.ravel(axs)):\n",
    "    sns.barplot(data=suites_df[suites_df.model_name == baseline_model], x=\"suite\", y=\"correct\", ax=ax)\n",
    "    plt.title(f\"{baseline_model} test suite results, averaged across corpus size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 7))\n",
    "sns.barplot(data=suites_df.reset_index(), x=\"model_name\", y=\"correct\")\n",
    "\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20, 10))\n",
    "sns.barplot(data=joined_data_circuits, x=\"circuit\", y=\"correct\", hue=\"model_name\", hue_order=model_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlled evaluation of model type + dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlled_suites_df = suites_df[suites_df.model_name.isin(controlled_models)]\n",
    "controlled_suites_df_mod = suites_df_mod[suites_df_mod.model_name.isin(controlled_models)]\n",
    "controlled_joined_data_circuits = joined_data_circuits[joined_data_circuits.model_name.isin(controlled_models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SG deltas w.r.t. test suite mean rather than absolute values.\n",
    "# This makes for a more easily interpretable visualization\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.axhline(0, c=\"gray\", alpha=0.5, linestyle=\"--\")\n",
    "sns.barplot(data=controlled_suites_df.reset_index(), x=\"model_name\", y=\"correct_delta\", order=controlled_model_order, ax=ax)\n",
    "sns.swarmplot(data=controlled_suites_df.reset_index(), x=\"model_name\", y=\"correct_delta\", order=controlled_model_order, alpha=0.7, ax=ax)\n",
    "\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Delta from per-suite mean accuracy\")\n",
    "plt.title(\"Model averages: delta from mean accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15, 10))\n",
    "# Estimate error intervals with a structured bootstrap: resampling units = model\n",
    "sns.barplot(data=controlled_suites_df.reset_index(), x=\"corpus\", y=\"correct_delta\",\n",
    "            units=\"model_name\", order=corpus_order)\n",
    "\n",
    "plt.xlabel(\"Corpus\")\n",
    "plt.ylabel(\"Delta from per-suite mean accuracy\")\n",
    "plt.title(\"Corpus averages: delta from mean accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20, 10))\n",
    "sns.barplot(data=controlled_joined_data_circuits, x=\"circuit\", y=\"correct_delta\", hue=\"model_name\", hue_order=controlled_model_order)\n",
    "\n",
    "# TODO swarmplot split across corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20, 10))\n",
    "sns.barplot(data=controlled_joined_data_circuits, x=\"circuit\", y=\"correct_delta\", hue=\"corpus\", units=\"model_name\", hue_order=corpus_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stability to modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlled_suites_df_mod.suite.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15, 10))\n",
    "sns.barplot(data=controlled_suites_df_mod, x=\"model_name\", y=\"correct\", hue=\"has_modifier\", order=controlled_model_order)\n",
    "plt.title(\"Stability to modification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15, 10))\n",
    "sns.barplot(data=controlled_suites_df_mod, x=\"corpus\", y=\"correct\", hue=\"has_modifier\", units=\"model_name\", order=corpus_order)\n",
    "plt.title(\"Stability to modification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=controlled_suites_df_mod, col=\"model_name\", height=7)\n",
    "g.map(sns.barplot, \"corpus\", \"correct\", \"has_modifier\", order=corpus_order, hue_order=[False, True])\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mod_results = controlled_suites_df_mod.groupby([\"model_name\", \"test_suite_base\", \"has_modifier\"]).correct.agg(correct=\"mean\").sort_index()\n",
    "avg_mod_diffs = avg_mod_results.xs(True, level=\"has_modifier\") - avg_mod_results.xs(False, level=\"has_modifier\")\n",
    "\n",
    "plt.subplots(figsize=(15, 10))\n",
    "sns.boxplot(data=avg_mod_diffs.reset_index(), x=\"model_name\", y=\"correct\", order=controlled_model_order)\n",
    "plt.title(\"Change in accuracy due to modification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy vs perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.scatterplot(data=joined_data, x=\"test_ppl\", y=\"correct\",\n",
    "                hue=\"model_name\", style=\"corpus\", s=150,\n",
    "                hue_order=model_order)\n",
    "plt.xlabel(\"Test corpus perplexity\")\n",
    "plt.ylabel(\"SyntaxGym absolute scores vs. perplexity\")\n",
    "plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "\n",
    "# Add horizontal lines for models without ppl estimates.\n",
    "no_ppl_data = joined_data[joined_data.test_ppl.isna()]\n",
    "for model_name, rows in no_ppl_data.groupby(\"model_name\"):\n",
    "    y = rows.correct.mean()\n",
    "    # TODO match legend color\n",
    "    # TODO show error region?\n",
    "    ax.axhline(y, linestyle=\"dashed\")\n",
    "    ax.text(110, y + 0.0025, model_name, alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.scatterplot(data=joined_data[joined_data.model_name.isin(controlled_models)], x=\"test_ppl\", y=\"correct_delta\",\n",
    "                hue=\"model_name\", style=\"corpus\", s=150,\n",
    "                hue_order=controlled_model_order)\n",
    "plt.xlabel(\"Test corpus perplexity\")\n",
    "plt.ylabel(\"SyntaxGym delta score\")\n",
    "plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "plt.title(\"SyntaxGym delta scores vs. perplexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lmplot(data=joined_data, x=\"test_ppl\", y=\"correct_delta\",\n",
    "               hue=\"corpus\", hue_order=corpus_order, truncate=True)\n",
    "g.ax.set_ylim((joined_data.correct_delta.min() - 0.1, joined_data.correct_delta.max() + 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lmplot(data=joined_data[joined_data.model_name.isin(controlled_models)], x=\"test_ppl\", y=\"correct_delta\",\n",
    "               hue=\"model_name\", hue_order=controlled_model_order, truncate=True)\n",
    "g.ax.set_ylim((joined_data.correct_delta.min() - 0.1, joined_data.correct_delta.max() + 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=joined_data_circuits, col=\"circuit\", height=5)\n",
    "g.map(sns.scatterplot, \"test_ppl\", \"correct\", \"model_name\",\n",
    "      hue_order=model_order, s=100)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=controlled_joined_data_circuits, col=\"circuit\", row=\"model_name\", height=5)\n",
    "g.map(sns.scatterplot, \"test_ppl\", \"correct\",\n",
    "      hue_order=controlled_model_order, s=100)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Item-level statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_FROM_ITEM_ANALYSIS = [\"random\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item-level prediction correlations across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_predictions = results_df[~results_df.model_name.isin(EXCLUDE_FROM_ITEM_ANALYSIS)] \\\n",
    "    .set_index([\"suite\", \"item\"]).sort_index().groupby([\"model_name\", \"corpus\", \"seed\"]).correct.apply(np.array)\n",
    "model_correlations, model_agreement = [], []\n",
    "for k1, k2 in itertools.combinations(list(item_predictions.index), 2):\n",
    "    k1_key = \" \".join(map(str, k1))\n",
    "    k2_key = \" \".join(map(str, k2))\n",
    "    k1_vals = item_predictions.loc[k1]\n",
    "    k2_vals = item_predictions.loc[k2]\n",
    "    \n",
    "    model_correlations.append((k1_key, *k1, k2_key, *k2, stats.spearmanr(k1_vals, k2_vals)[0]))\n",
    "    model_agreement.append((k1_key, *k1, k2_key, *k2, (k1_vals == k2_vals).mean()))\n",
    "\n",
    "corr_df = pd.DataFrame(model_correlations, columns=[\"key_1\", \"model_1\", \"corpus_1\", \"seed_1\", \"key_2\", \"model_2\", \"corpus_2\", \"seed_2\", \"corr\"])\n",
    "agree_df = pd.DataFrame(model_agreement, columns=[\"key_1\", \"model_1\", \"corpus_1\", \"seed_1\", \"key_2\", \"model_2\", \"corpus_2\", \"seed_2\", \"agreement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(data=corr_df.pivot(\"key_1\", \"key_2\", \"corr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(data=agree_df.pivot(\"key_1\", \"key_2\", \"agreement\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 10))\n",
    "sns.distplot(results_df[~results_df.model_name.isin(EXCLUDE_FROM_ITEM_ANALYSIS)].groupby([\"suite\", \"item\"]).correct.agg(\"mean\"), bins=20)\n",
    "plt.title(\"Distribution of item-level accuracy means\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 10))\n",
    "sns.distplot(results_df[~results_df.model_name.isin(EXCLUDE_FROM_ITEM_ANALYSIS)].groupby([\"suite\", \"item\"]).correct.agg(\"std\"), bins=20)\n",
    "plt.title(\"Distribution of item-level accuracy stdevs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get items for which all models fail / succeed\n",
    "all_fail = results_df[~results_df.model_name.isin(EXCLUDE_FROM_ITEM_ANALYSIS) & results_df.model_name.isin(controlled_models)].groupby([\"suite\", \"item\"]).correct.max() == False\n",
    "all_succeed = results_df[~results_df.model_name.isin(EXCLUDE_FROM_ITEM_ANALYSIS) & results_df.model_name.isin(controlled_models)].groupby([\"suite\", \"item\"]).correct.min() == True\n",
    "\n",
    "# Get items for which each condition is true\n",
    "all_fail = all_fail[all_fail]\n",
    "all_succeed = all_succeed[all_succeed]\n",
    "\n",
    "print(\"All fail\\n\", all_fail)\n",
    "print(\"All succeed\\n\", all_succeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 10))\n",
    "sns.distplot(suites_df[~suites_df.model_name.isin(EXCLUDE_FROM_ITEM_ANALYSIS)].correct, bins=20)\n",
    "plt.title(\"Distribution of suite-level accuracy means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Circuitâ€“circuit correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude some models from circuit correlation analysis.\n",
    "EXCLUDE_FROM_CIRCUIT_ANALYSIS = [\"random\", \"ngram\", \"1gram\", \"ngram-single\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(len(circuit_order), len(circuit_order), figsize=(25, 25))\n",
    "plt.subplots_adjust(hspace=0.6, wspace=0.6)\n",
    "\n",
    "source_df = suites_df[~suites_df.model_name.isin(EXCLUDE_FROM_CIRCUIT_ANALYSIS)]\n",
    "\n",
    "for c1, row in zip(circuit_order, axs):\n",
    "    for c2, ax in zip(circuit_order, row):\n",
    "        if c1 <= c2:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "            \n",
    "        xs = source_df[source_df.circuit == c1].groupby([\"model_name\", \"corpus\", \"seed\"]).correct.agg({c1: \"mean\"})\n",
    "        ys = source_df[source_df.circuit == c2].groupby([\"model_name\", \"corpus\", \"seed\"]).correct.agg({c2: \"mean\"})\n",
    "        df = pd.concat([xs, ys], axis=1)\n",
    "        ax.set_title(\"%s /\\n %s\" % (c1, c2))\n",
    "        sns.regplot(data=df, x=c1, y=c2, ax=ax)\n",
    "        \n",
    "plt.suptitle(\"Circuit--circuit correlations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate lower-bound Spearman r for each circuit-circuit relation\n",
    "# by running a structured bootstrap over model--corpus--seeds: randomly\n",
    "# resample model--corpus--seed combinations and recompute Spearman r's.\n",
    "def estimate_r(xs):\n",
    "    # Calculate Spearman-r on bootstrap sample comparing two circuits (shape n * 2)\n",
    "    corr, pval = stats.spearmanr(xs[:, 0], xs[:, 1])\n",
    "    return corr\n",
    "\n",
    "corr_data = pd.DataFrame(index=circuit_order, columns=circuit_order)\n",
    "n_boot = 500\n",
    "for c1, c2 in tqdm(list(itertools.combinations(circuit_order, 2))):\n",
    "    xs = source_df[source_df.circuit == c1].groupby([\"model_name\", \"corpus\", \"seed\"]).correct.agg({c1: \"mean\"})\n",
    "    ys = source_df[source_df.circuit == c2].groupby([\"model_name\", \"corpus\", \"seed\"]).correct.agg({c2: \"mean\"})\n",
    "\n",
    "    df = pd.concat([xs, ys], axis=1)\n",
    "    # Concatenate model--corpus--seed labels to make structured bootstrapping easier.\n",
    "    df[\"model_key\"] = [\" \".join(map(str, key)) for key in df.index.tolist()]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    corr_data.loc[c1, c2] = sns.utils.ci(sns.algorithms.bootstrap(df, units=df.model_key, n_boot=n_boot, func=estimate_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative tests\n",
    "\n",
    "`SG ~ ppl:corpus + model_name + (1 | test_suite)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_map = perplexity_df.test_ppl.to_dict()\n",
    "\n",
    "reg_df = results_df[results_df.model_name.isin(controlled_models)].copy()\n",
    "reg_df[\"test_ppl\"] = pd.Series(list(zip(reg_df.model_name, reg_df.corpus, reg_df.seed))).map(perplexity_map)\n",
    "reg_df.to_csv(\"reg_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
